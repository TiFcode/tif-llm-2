{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TiFcode/tif-llm-2/blob/main/TiFcode_Chat_With_Your_Documents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TiFcode - Chat With Your Documents"
      ],
      "metadata": {
        "id": "vY4vg3ZCEhVP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we get the secret Google GenAI API key from the Google Colab Secrets:"
      ],
      "metadata": {
        "id": "FVZhHRo7etTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata as GoogleColabSecretUserData\n",
        "SECRET_GOOGLE_GENAI_API_KEY=GoogleColabSecretUserData.get(\"GoogleGenAiApiKey\")"
      ],
      "metadata": {
        "id": "sHlbxALhDyYY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl ipinfo.io"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYNC83lIT243",
        "outputId": "74ae9ce5-d523-4f91-af35-e942de63a486"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"ip\": \"35.196.60.206\",\n",
            "  \"hostname\": \"206.60.196.35.bc.googleusercontent.com\",\n",
            "  \"city\": \"North Charleston\",\n",
            "  \"region\": \"South Carolina\",\n",
            "  \"country\": \"US\",\n",
            "  \"loc\": \"32.8546,-79.9748\",\n",
            "  \"org\": \"AS396982 Google LLC\",\n",
            "  \"postal\": \"29415\",\n",
            "  \"timezone\": \"America/New_York\",\n",
            "  \"readme\": \"https://ipinfo.io/missingauth\"\n",
            "}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmnLf5_zFBly",
        "outputId": "d2e88185-0039-4c61-8f28-afaba20fa11b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade google-generativeai langchain-google-genai langchain pypdf chromadb"
      ],
      "metadata": {
        "id": "aADZnXXjaAlX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai"
      ],
      "metadata": {
        "id": "1Jb6x0ejbyyx"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains.question_answering import load_qa_chain"
      ],
      "metadata": {
        "id": "Qj5vx96BDolc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "import textwrap\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "BuRF1CYodZg6"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "google.generativeai.configure(api_key=SECRET_GOOGLE_GENAI_API_KEY)"
      ],
      "metadata": {
        "id": "jXiI5LbueVCt"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we list the names of the generative models that are available in the API, as we need to select one of them and configure a string with the name of the selected model:"
      ],
      "metadata": {
        "id": "hxiJLCLYEppL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ai_model in google.generativeai.list_models():\n",
        "  if 'generateContent' in ai_model.supported_generation_methods:\n",
        "    print(ai_model.name)"
      ],
      "metadata": {
        "id": "snAZIbwzf68u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "28fd090c-78d4-4211-c3ae-89e96a7439f7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.0-pro\", temperature=0.3, google_api_key=SECRET_GOOGLE_GENAI_API_KEY)"
      ],
      "metadata": {
        "id": "FtGMQNdWB6ga"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare the PDF documents that we will chat on with the AI"
      ],
      "metadata": {
        "id": "JzZSAiNZIpsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pdfs\n",
        "!gdown 1v94CNKi1truPO2M1E0l1VTDGxiqdRcRk -O \"pdfs/OntoSense - Whitepaper (version 2018_01_06__11_09).pdf\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocVcFB8LCKxM",
        "outputId": "d527139f-ae90-4c17-d0e9-21b10f099ba0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘pdfs’: File exists\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1v94CNKi1truPO2M1E0l1VTDGxiqdRcRk\n",
            "To: /content/pdfs/OntoSense - Whitepaper (version 2018_01_06__11_09).pdf\n",
            "100% 1.66M/1.66M [00:00<00:00, 89.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document_loader = PyPDFDirectoryLoader(\"pdfs\")\n",
        "loaded_pages = document_loader.load_and_split()"
      ],
      "metadata": {
        "id": "b-ZkuUmmJ6Gi"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of loaded pages:\", len(loaded_pages))\n",
        "\n",
        "# Length of each loaded page\n",
        "# *** TBD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks5YLET3K2qu",
        "outputId": "1660a9f4-adc2-4665-c1a6-59eca98de94e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of loaded pages: 106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(\"The first loaded page is:\\n\\n\", loaded_pages[0].page_content)"
      ],
      "metadata": {
        "id": "fpsMr8o0KlWX"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare the text chunks"
      ],
      "metadata": {
        "id": "n-b0ic2CcY0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_text_from_loaded_pages = \"\\n\\n\".join(str(loaded_page.page_content) for loaded_page in loaded_pages)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=200)\n",
        "\n",
        "text_chunks = text_splitter.split_text(all_text_from_loaded_pages)"
      ],
      "metadata": {
        "id": "4NfqmaWOMXeU"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of text chunks:\", len(text_chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7-QrzHrZ7If",
        "outputId": "61b42ce7-e3bb-42bc-8454-247dbbc46cfd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of text chunks: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(\"The first text chunk is:\\n\\n[\",text_chunks[0],\"]\")"
      ],
      "metadata": {
        "id": "b-hTgukNar8-"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compute the embeddings of the text chunks"
      ],
      "metadata": {
        "id": "jWTCUb_Abyf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "google_genai_embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\", google_api_key=SECRET_GOOGLE_GENAI_API_KEY)\n",
        "chroma_vector_store = Chroma.from_texts(text_chunks, google_genai_embeddings).as_retriever()"
      ],
      "metadata": {
        "id": "Z2SKl0uAbZ8t"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chatting"
      ],
      "metadata": {
        "id": "ogAac3yFfk2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what is CR and why is it useful\"\n",
        "relevant_text_chunks = chroma_vector_store.get_relevant_documents(question)"
      ],
      "metadata": {
        "id": "39GiOE1bdgQn"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of relevant text chunks:\", len(relevant_text_chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kRC2Y62eTiD",
        "outputId": "45d8b9b9-a9bd-4ca1-bf7f-b1610b7afabc"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of relevant text chunks: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#relevant_text_chunks[0]"
      ],
      "metadata": {
        "id": "jCZNFHvAiw6g"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"\n",
        "  Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in provided context just say \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
        "  Context:\\n {context}\\n\n",
        "  Question:\\n {param_question}?\\n\n",
        "\n",
        "  Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template = prompt_template, input_variables = [\"context\",\"param_question\"])"
      ],
      "metadata": {
        "id": "7yCbgrQVfXqh"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=prompt)\n",
        "response = chain(\n",
        "    {\"input_documents\":relevant_text_chunks,\n",
        "     \"param_question\":question},\n",
        "     return_only_outputs=True\n",
        ")"
      ],
      "metadata": {
        "id": "AxjBFMRIgN59"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The question was:\\n\\n[\",question,\"]\\n\\n\")\n",
        "text_to_format = response['output_text']\n",
        "formatted_text = textwrap.fill(text_to_format, width=80)\n",
        "print(\"The response is:\\n\\n[\",formatted_text,\"]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKRVp903nXA-",
        "outputId": "7a13d07b-b7d1-4a88-b74e-7cfb4119b153"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The question was:\n",
            "\n",
            "[ what is CR and why is it useful ]\n",
            "\n",
            "\n",
            "The response is:\n",
            "\n",
            "[ CR stands for Confidence Rank. It is a measure of the confidence of the system\n",
            "in using a specific concept/node/edge (denoted c new).   It is useful because it\n",
            "allows the system to evaluate the importance of a concept/node/edge based on its\n",
            "source, references, goals, and feedback. This information can then be used to\n",
            "make decisions about which concepts/nodes/edges to use in the system. ]\n"
          ]
        }
      ]
    }
  ]
}